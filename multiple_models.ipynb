{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d64b4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages needed\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "# from google import genai\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8fc57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e63bf942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins with sk-ant-\n",
      "Google API Key exists and begins with AI\n",
      "DeepSeek API Key exists and begins with sk-\n",
      "Groq API Key exists and begins with gsk_\n"
     ]
    }
   ],
   "source": [
    "# Set the API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins with {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins with {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins with {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins with {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4777311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question is: Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.\n"
     ]
    }
   ],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n",
    "\n",
    "print(f\"The question is: {messages[0]['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b58f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "If you were tasked with designing an entirely new ethical framework to govern the development and implementation of artificial intelligence, what core principles would you prioritize, and how would you address potential conflicts between those principles in practical scenarios?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you were tasked with designing an entirely new ethical framework to govern the development and implementation of artificial intelligence, what core principles would you prioritize, and how would you address potential conflicts between those principles in practical scenarios?\n"
     ]
    }
   ],
   "source": [
    "openai_client = OpenAI()\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "\n",
    "question = response.choices[0].message.content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d541d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 4o mini\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "display(Markdown(answer))\n",
    "\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic Claude 3.7 Sonnet\n",
    "anthropic_client = Anthropic()\n",
    "model_name = \"claude-3-7-sonnet-20250219\"\n",
    "\n",
    "response = anthropic_client.messages.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "answer = response.content[0].text\n",
    "display(Markdown(answer))\n",
    "\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to delete the second answer because I ran the code twice. \n",
    "# Cleaned it up without rerunning the entire notebook again.\n",
    "#\n",
    "\n",
    "# print(competitors)\n",
    "\n",
    "# from IPython.display import Markdown, display\n",
    "\n",
    "# for idx, answer in enumerate(answers, 1):\n",
    "#     display(Markdown(f\"**Answer {idx}:**\\n\\n{answer}\\n\\n---\"))\n",
    "\n",
    "# del competitors[1]\n",
    "# del answers[1]\n",
    "\n",
    "# print(competitors)\n",
    "# for idx, answer in enumerate(answers, 1):\n",
    "#     display(Markdown(f\"**Answer {idx}:**\\n\\n{answer}\\n\\n---\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7620d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how the Google client has to be called but I cannot import google.generativeai\n",
    "# as it is not installed in the environment.\n",
    "#\n",
    "# gemini_client = GoogleGenerativeAI()\n",
    "# model_name = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "# response = gemini_client.generate_content(\n",
    "#     model=model_name,\n",
    "#     contents=messages\n",
    "# )\n",
    "\n",
    "# answer = response.text\n",
    "# display(Markdown(answer))\n",
    "\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)\n",
    "\n",
    "gemini_client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini_client.chat.completions.create(\n",
    "    model=model_name, \n",
    "    messages=messages\n",
    ")\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek Chat\n",
    "\n",
    "deepseek_client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek_client.chat.completions.create(\n",
    "    model=model_name, \n",
    "    messages=messages\n",
    ")\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq \n",
    "\n",
    "groq_client = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq_client.chat.completions.create(\n",
    "    model=model_name, \n",
    "    messages=messages, \n",
    "    temperature=0.7\n",
    ")\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb79329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama from local machine\n",
    "# Currently llama3.2:latest is being used. \n",
    "# We can also pull deepseek, Gemma, Phi, Qwen, or DeepSeek\n",
    "\n",
    "ollama_client = OpenAI(api_key=ollama_api_key, base_url=\"http://localhost:11434/v1\")\n",
    "model_name = \"llama3.2:latest\"\n",
    "\n",
    "response = ollama_client.chat.completions.create(\n",
    "    model=model_name, \n",
    "    messages=messages, \n",
    "    temperature=0.7\n",
    ")\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80058274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'claude-3-7-sonnet-20250219', 'gemini-2.0-flash', 'deepseek-chat', 'llama-3.3-70b-versatile', 'llama3.2:latest']\n"
     ]
    }
   ],
   "source": [
    "# Let's list all the competitors that we have used so far\n",
    "print(competitors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d042aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets zip the competitors and answers\n",
    "competitors_answers = list(zip(competitors, answers))\n",
    "\n",
    "# lets print the competitors and answers\n",
    "for competitor, answer in competitors_answers:\n",
    "    print(f\"{competitor}: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a009387",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(competitors_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fda8dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1} - {competitors[index]}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\"\n",
    "\n",
    "# display(Markdown(together))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a697e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02929142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the judge agent and ask it to judge the responses\n",
    "judge_messages = [\n",
    "    {\"role\": \"user\", \"content\": judge}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ffd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"3\", \"1\", \"5\", \"4\", \"2\", \"6\"]}\n"
     ]
    }
   ],
   "source": [
    "# Lets use o3-mini to judge the responses\n",
    "\n",
    "judge_response = openai_client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages\n",
    ")\n",
    "\n",
    "results = judge_response.choices[0].message.content\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06930a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gemini-2.0-flash\n",
      "Rank 2: gpt-4o-mini\n",
      "Rank 3: llama-3.3-70b-versatile\n",
      "Rank 4: deepseek-chat\n",
      "Rank 5: claude-3-7-sonnet-20250219\n",
      "Rank 6: llama3.2:latest\n"
     ]
    }
   ],
   "source": [
    "# Lets convert the json into results\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
