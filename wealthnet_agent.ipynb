{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e353861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "from IPython.display import Markdown\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bf69f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the environment\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_client = OpenAI()\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "# ollama_client = OpenAI(api_key=ollama_api_key, base_url=\"http://localhost:11434/v1\")\n",
    "# model_name = \"llama3.2:latest\"\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "# ]\n",
    "\n",
    "# response = ollama_client.chat.completions.create(model=model_name, messages=messages, temperature=0.7)\n",
    "\n",
    "# answer = response.choices[0].message.content\n",
    "# display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ae585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushover configs\n",
    "\n",
    "pushover_user = os.getenv(\"PUSHOVER_USER\")\n",
    "pushover_token = os.getenv(\"PUSHOVER_TOKEN\")\n",
    "pushover_url = f\"https://api.pushover.net/1/messages.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to send a pushover notification\n",
    "\n",
    "def push(message):\n",
    "    print(f\"Push {message}\")\n",
    "    payload = {\n",
    "        \"token\": pushover_token,\n",
    "        \"user\": pushover_user,\n",
    "        \"message\": message\n",
    "    }\n",
    "    response = requests.post(pushover_url, data=payload)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64333cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "push(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d72f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tool that the LLM can use to record if a user is interested in knowing more\n",
    "def record_user_details(email, name=\"Name not provided\", notes=\"not provided\"):\n",
    "    push(f\"User {name} with email {email} is interested in knowing more. Notes: {notes}\")\n",
    "    return {\"Recorded\": \"ok\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408c79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tool that the LLM can use to record if it doesn't have the answer\n",
    "def record_unknown_question(question):\n",
    "    push(f\"recording {question} asked that I couldn't answer\")\n",
    "    return {\"Recorded\": \"ok\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82361e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_user_details_json = {\n",
    "    \"name\": \"record_user_details\",\n",
    "    \"description\": \"Use this tool to record that a user is interested in knowing more about a topic and provided an email address\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"email\": {\n",
    "                \"type\": \"string\", \"description\": \"The email address of the user\"\n",
    "            },\n",
    "            \"name\": {\n",
    "                \"type\": \"string\", \"description\": \"The name of the user\"\n",
    "            },\n",
    "            \"notes\": {\n",
    "                \"type\": \"string\", \"description\": \"Any additional notes about the user\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"email\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd586d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_unknown_question_json = {\n",
    "    \"name\": \"record_unknown_question\",\n",
    "    \"description\": \"Use this tool to record the question that a user asked which you couldn't answer\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\", \n",
    "                \"description\": \"The question that the user asked which you couldn't answer\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c56a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#createa list of tools that are available to the LLM\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": record_user_details_json\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": record_unknown_question_json\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9630e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e45086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can take a list of tool calls and run them. \n",
    "\n",
    "# def handle_tool_calls(tool_calls):\n",
    "#     results = []\n",
    "#     for tool_call in tool_calls:\n",
    "#         tool_name = tool_call.function.name\n",
    "#         arguments = json.loads(tool_call.function.arguments)\n",
    "#         print(f\"Tool call: {tool_name}\", flush=True)\n",
    "\n",
    "#         if tool_name == \"record_user_details\":\n",
    "#             record_user_details(**arguments)\n",
    "#         elif tool_name == \"record_unknown_question\":\n",
    "#             record_unknown_question(**arguments)\n",
    "        \n",
    "#         results.append({\n",
    "#             \"role\": \"tool\", \n",
    "#             \"content\": json.dumps(results),\n",
    "#             \"tool_call_id\": tool_call.id\n",
    "#             }\n",
    "#         )\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a more elegant way that avoids the IF statement.\n",
    "\n",
    "def handle_tool_calls(tool_calls):\n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        print(f\"Tool called: {tool_name}\", flush=True)\n",
    "        \n",
    "        tool = globals().get(tool_name)\n",
    "        result = tool(**arguments) if tool else {}\n",
    "        results.append({\"role\": \"tool\",\"content\": json.dumps(result),\"tool_call_id\": tool_call.id})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd10c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will work better if the book is vectorized for ollama. Currently, it uses too many tokens and is unable to answer questions.\n",
    "\n",
    "# reader = PdfReader(\"knowledgebase/peng_book.pdf\")\n",
    "# peng_book = \"\"\n",
    "# for page in reader.pages:\n",
    "#     text = page.extract_text()\n",
    "#     if text:\n",
    "#         peng_book += text\n",
    "\n",
    "# with open(\"knowledgebase/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     summary = f.read()\n",
    "\n",
    "# name = \"Gautham Pallapa\"\n",
    "# book_title = \"Mastering Enterprise Platform Engineering\"\n",
    "\n",
    "# Using the LinkedIn profile as the book needs to be vectorized for ollama to really work well\n",
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "\n",
    "name = \"Gautham Pallapar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the system prompt for the book landing page\n",
    "\n",
    "# system_prompt = f\"You are acting as {name}. You are answering questions for {name}'s book's landing page, \\\n",
    "# particularly questions related to {name}'s book titled: {book_title}. \\\n",
    "# Your responsibility is to represent {name} for interactions on the book's landing page as faithfully as possible. \\\n",
    "# You are given a summary of {name}'s background and the complete text of {book_title} which you can use to answer questions. \\\n",
    "# Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "# If you don't know the answer to any question, use your record_unknown_question tool to record the question that you couldn't answer, even if it's about something trivial or unrelated to career. \\\n",
    "# If the user is engaging in discussion, try to steer them towards getting in touch via email; ask for their email and record it using your record_user_details tool. \"\n",
    "\n",
    "# system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## Book Title:\\n{book_title}\\n\\n## Book Text:\\n{peng_book}\\n\"\n",
    "# system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n",
    "\n",
    "# I am creating a system prompt for the website and the LinkedIn profile\n",
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer to any question, use your record_unknown_question tool to record the question that you couldn't answer, even if it's about something trivial or unrelated to career. \\\n",
    "If the user is engaging in discussion, try to steer them towards getting in touch via email; ask for their email and record it using your record_user_details tool. \"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcd8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    print(messages)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Call to the LLM with the tools json\n",
    "        # If I want to call ollama, I just need to change the client to ollama_client and set the model name to the ollama model name\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model_name, \n",
    "            messages=messages, \n",
    "            temperature=0.7, \n",
    "            tools=tools\n",
    "        )\n",
    "        \n",
    "        finish_reason = response.choices[0].finish_reason\n",
    "        \n",
    "        # If the LLM wants to call a tool, we do that\n",
    "         \n",
    "        if finish_reason==\"tool_calls\":\n",
    "            message = response.choices[0].message\n",
    "            tool_calls = message.tool_calls\n",
    "            results = handle_tool_calls(tool_calls)\n",
    "            messages.append(message)\n",
    "            messages.extend(results)\n",
    "        else:\n",
    "            done = True\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
